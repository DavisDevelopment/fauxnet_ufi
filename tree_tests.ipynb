{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from functools import partial\n","from math import floor\n","from random import sample\n","from pprint import pprint\n","import random\n","import numpy as np\n","import pandas as pd\n","# from numpy import typename\n","import torch\n","from torch import Tensor, tensor, typename\n","from faux.pgrid import ls"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[4m\u001b[34mdatatools.py:19\u001b[0m /home/ryan/Documents/Development\n"]}],"source":["from coretools import list_stonks, load_frame"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from nn.trees.binary_tree import TorchDecisionTreeRegressor, TorchDecisionTreeClassifier\n","from nn.trees.random_forest import TorchRandomForestClassifier, TorchRandomForestRegressor"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from typing import *\n","from fn import F, _\n","from cytoolz import *"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from pandas import DataFrame, Series\n","from tools import flatten_dict, hasmethod, unzip, maxby, Struct"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import gc\n","from tqdm import tqdm\n","import pickle "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import sys, os\n","P = os.path\n","from time import sleep, time\n","from olbi import printing, configurePrinting\n","import faux.backtesting.common as fauxbt_common\n","from faux.backtesting.common import split_samples"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from tsai.models.ROCKET import RocketClassifier\n","from tsai.models.MINIROCKET import MiniRocketClassifier, MiniRocketVotingClassifier"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["samples_for2 = fauxbt_common.samples_for"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def scale_dataframe(df:DataFrame, nq=500):\n","   from sklearn.preprocessing import QuantileTransformer\n","   scaler = QuantileTransformer(n_quantiles=nq)\n","   r = df.copy()\n","   # print(df)\n","   sX = scaler.fit_transform(X=df).T\n","   for i, c in enumerate(r.columns.tolist()):\n","      r[c] = sX[i]\n","   return r"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def test_indicator_config(symbol, indicators, config={}):\n","   from sklearn.metrics import accuracy_score\n","   \n","   # indicators = mkAnalyzer(items)\n","   indicators = F(indicators.apply)\n","   \n","   # df = load_frame(symbol)\n","   # print(indicators(df))\n","   X, y = samples_for2(symbol, indicators, xcols_not=['open', 'high', 'low', 'close', 'datetime'])\n","   X = torch.from_numpy(X)\n","   y = torch.from_numpy(y)\n","   train_X, train_y, test_X, test_y = split_samples(X, y, 0.83)\n","   \n","   from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n","   from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n","   from sklearn.neural_network import MLPClassifier\n","   # from tsai.models import MINIROCKET\n","   \n","   models = [\n","      DecisionTreeClassifier(criterion='gini'),\n","      DecisionTreeClassifier(criterion='entropy'),\n","      DecisionTreeClassifier(criterion='log_loss'),\n","      RandomForestClassifier(n_estimators=100, criterion='entropy', n_jobs=-1),\n","      RandomForestClassifier(n_estimators=100, criterion='gini', n_jobs=-1),\n","      RandomForestClassifier(n_estimators=100, criterion='log_loss', n_jobs=-1),\n","      HistGradientBoostingClassifier(),\n","      ExtraTreeClassifier(criterion='entropy', splitter='best'),\n","      ExtraTreesClassifier(n_estimators=200, criterion='entropy', n_jobs=-1),\n","      \n","      MLPClassifier(hidden_layer_sizes=(120, 70, 32), learning_rate='adaptive'),\n","   ]\n","   \n","   results = []\n","   for model in models:\n","      # print(f'Fitting {typename(model)} on {len(train_X)} samples...')\n","      # print(train_X.shape, train_y.shape)\n","      model.fit(train_X, train_y)\n","      \n","      y_pred = model.predict(test_X)\n","      \n","      acc = 100.0 * accuracy_score(\n","         test_y.numpy(),\n","         y_pred\n","      )\n","      \n","      results.append(dict(\n","         model_type=type(model),\n","         model_args=model.get_params(),\n","         indicators=indicators,\n","         accuracy=acc\n","      ))\n","      # print(f'{typename(model)} Accuracy: {acc:.2f}%')\n","      \n","   results = DataFrame.from_records(results)\n","   results = results.sort_values('accuracy', ascending=False, ignore_index=True)\n","   results = results[['accuracy', 'model_args', 'model_type', 'indicators']]\n","   print(results)\n","   \n","   return results.iloc[0]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def test_minirocket(data=None, n_estimators=1):\n","   from tsai.models.MINIROCKET import MiniRocketClassifier, MiniRocketVotingClassifier\n","   # from tsai.models.MINIROCKET_\n","   from tsai.basics import get_UCR_data, timer\n","   \n","   if data is None:\n","      # Univariate classification with sklearn-type API\n","      dsid = 'OliveOil'\n","      X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)   # Download the UCR dataset\n","   \n","   else:\n","      X_train, y_train, X_valid, y_valid = data\n","   \n","   # print(X_train.shape)\n","   # print(y_train)\n","   \n","   # Computes MiniRocket features using the original (non-PyTorch) MiniRocket code.\n","   # It then sends them to a sklearn's RidgeClassifier (linear classifier).\n","   model = (MiniRocketClassifier() if n_estimators == 1 else MiniRocketVotingClassifier(n_estimators=n_estimators, n_jobs=1))\n","   \n","   timer.start(False)\n","   model.fit(X_train, y_train)\n","   t = timer.stop()\n","   \n","   acc = model.score(X_valid, y_valid)\n","   # MiniRocketClassifier()\n","   print(f'valid accuracy    : {acc:.3%} time: {t}')\n","   # del model\n","   \n","   return dict(\n","      accuracy=acc,\n","      model=model\n","   )"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["   \n","def fittest(data=None, ctor=MiniRocketClassifier, **ctor_kw):\n","   # from tsai.models.MINIROCKET_\n","   from tsai.basics import get_UCR_data, timer\n","   \n","   if data is None:\n","      # Univariate classification with sklearn-type API\n","      dsid = 'OliveOil'\n","      X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)   # Download the UCR dataset\n","   \n","   else:\n","      X_train, y_train, X_valid, y_valid = data\n","   \n","   # print(X_train.shape)\n","   # print(y_train)\n","   \n","   # Computes MiniRocket features using the original (non-PyTorch) MiniRocket code.\n","   # It then sends them to a sklearn's RidgeClassifier (linear classifier).\n","   model = ctor(**ctor_kw)\n","   \n","   timer.start(False)\n","   model.fit(X_train, y_train)\n","   t = timer.stop()\n","   \n","   acc = model.score(X_valid, y_valid)\n","   print('\\n'.join([\n","      f'valid accuracy  :  {acc:.3%}',\n","      f'time            :  {t}'\n","   ]))\n","   # del model\n","   \n","   return dict(\n","      accuracy=acc,\n","      model=model\n","   )"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def wrapped_minirockets(self, X:Tensor):\n","   if hasmethod(self, 'predict'):\n","      model = self\n","   elif isinstance(self, dict) and 'model' in self:\n","      model = self['model']\n","   else:\n","      raise ValueError('invalid self')\n","   if X.ndim == 2:\n","      X = X.unsqueeze(0)\n","   elif X.ndim == 3:\n","      pass\n","   \n","   # assert X.shape[2] == winning_params['seq_len'], f'expected {winning_params[\"seq_len\"]}, but got {X.shape[2]}'\n","   \n","   ypred = model.predict(X.numpy())\n","   \n","   return torch.from_numpy(ypred)      "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def dcpy(target, src, *keys):\n","   for k in keys:\n","      target[k] = src[k]\n","      \n","from coinflip_backtest import backtest\n","from time import sleep\n","      \n","def backtest_minirockets(symbol, ts_idx, test_X, test_y, model, n_eval_days=90):\n","   totlist = lambda a: [a[i] for i in range(len(a))]\n","         \n","   slbeg, slend = -(n_eval_days * 2), -n_eval_days\n","   \n","   if n_eval_days < len(test_X):\n","      times, bX, by = (\n","         ts_idx[-n_eval_days:],\n","         test_X[len(test_X) - n_eval_days - 1:],\n","         test_y[len(test_X) - n_eval_days - 1:],\n","      )\n","   else:\n","      times = ts_idx[:]\n","      bX = test_X[:]\n","      by = test_y[:]\n","   \n","   wmodel = partial(wrapped_minirockets, model)\n","   btres = backtest(symbol, wmodel, samples=(times, bX, by), pos_type='long')\n","   \n","   return btres"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def cfgOptStep(train_symbol, hyperparams, data_transform, model=None, val_split=0.25, n_eval_days=365, unlink=True, backtest=True):\n","   ts_idx, X, y = samples_for2(\n","      symbol=train_symbol, \n","      analyze=F(data_transform.apply) if not callable(data_transform) else data_transform, \n","      xcols_not=['open', 'close', 'datetime'],\n","      x_timesteps=hyperparams['seq_len']\n","   )\n","   X = torch.from_numpy(X)\n","   y = torch.from_numpy(y)\n","   \n","   #* train the model on 85% of the available data, evaluating on the remaining 15%\n","   train_X, train_y, test_X, test_y = split_samples(X=X, y=y, pct=val_split, shuffle=False)\n","   train_X, test_X = tuple((v.unsqueeze(1) if v.ndim == 2 else v.swapaxes(1, 2)) for v in (train_X, test_X))\n","   \n","   if model is None:\n","      data = tuple(map(lambda v: v.numpy(), (train_X, train_y, test_X, test_y)))\n","      \n","      #best = test_minirocket(data=data, n_estimators=hyperparams['n_estimators'])\n","      best = fittest(data=data, ctor=MiniRocketClassifier)\n","      \n","      model = best['model']\n","      if unlink:\n","         del best['model']\n","      \n","   else:\n","      best = Struct()\n","   \n","   best['indicators'] = (data_transform if callable(data_transform) else data_transform.items())\n","   best['hyperparams'] = hyperparams\n","   \n","   if backtest:\n","      start_time = time()\n","      btres = backtest_minirockets(train_symbol, ts_idx, test_X, test_y, model, n_eval_days=n_eval_days)\n","      # print(f'took {time() - start_time}secs to run backtest')\n","      dcpy(best, btres, 'roi', 'baseline_roi', 'vs_market', 'pl_ratio')\n","   \n","   return best\n","   \n","def evaluateConfigurationOn(symbols:List[str], params, transform, model=None, val_split=0.25, n_eval_days=60):\n","   print(transform, params)\n","   entries = [\n","      cfgOptStep(sym, params, transform, model=model, val_split=val_split, n_eval_days=n_eval_days) for sym in symbols\n","   ]\n","   \n","   entries = [(e.asdict() if not isinstance(e, dict) else e) for e in entries]\n","   dfe:DataFrame = pd.DataFrame.from_records(entries)\n","   # print(dfe)\n","   \n","   res = Struct()\n","   for c, ctype in dfe.dtypes.to_dict().items():\n","      if ctype.name.startswith('float'):\n","         minc, meanc, maxc = (dfe[c].min(), dfe[c].mean(), dfe[c].max())\n","         res[c] = meanc\n","         res[f'min_{c}'] = minc\n","         res[f'max_{c}'] = maxc\n","      else:\n","         res[c] = dfe[c][0]\n","   \n","   print(res)\n","   return res"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def finetune_for(symbol:str, param_space, transforms, model_ctor=None, val_split=0.25, n_eval_days=365, n_winners=15):\n","   from tqdm import tqdm\n","   from faux.pgrid import PGrid\n","   from faux.features.ta.loadout import Indicators, IndicatorBag\n","   from cytoolz.itertoolz import topk\n","   \n","   #* prepare (hyper)parameter space\n","   if isinstance(param_space, PGrid):\n","      param_space = param_space.expand()\n","   elif isinstance(param_space, list):\n","      param_space = param_space\n","   elif isinstance(param_space, dict):\n","      param_space = [param_space]\n","   \n","   #* prepare space of possible analyses of the input data\n","   if isinstance(transforms, IndicatorBag):\n","      transforms = ls([x for x in transforms.sampling(4)], True)\n","   elif isiterable(transforms):\n","      transforms = list(transforms)\n","   else:\n","      transforms = [transforms]\n","      \n","   #* ensure that we know what type of model we'll be using\n","   if model_ctor is None:\n","      model_ctor = MiniRocketClassifier\n","   #* create progress bar, so we'll know where we're at\n","   pbar = tqdm(total=len(transforms) * len(param_space))\n","   \n","   attempts = []\n","   \n","   #TODO: replace this nested loop with some sort of non-brute-force grid search, using parallelism to accelerate computation\n","   for transform in transforms:\n","      for params in param_space:\n","         pbar.update()\n","         \n","         if model_ctor is None:\n","            model = model_ctor(**params)\n","         else:\n","            model = None\n","            \n","         res = cfgOptStep(symbol, params, transform, model=model, val_split=val_split, n_eval_days=n_eval_days)\n","         # res['params'] = params\n","         res['transform'] = transform\n","         \n","         attempts.append(res)\n","   winners = list(topk(n_winners, attempts, _['vs_market']))\n","   # print(winners[0])\n","   \n","   return winners"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def main():   \n","   from faux.pgrid import PGrid\n","   from faux.features.ta.loadout import Indicators, IndicatorBag\n","   \n","   last_run_path = 'minirocket_exp_results.pickle'\n","   best_config = None\n","   \n","   if P.exists(last_run_path):\n","      try:\n","         #* load last-run-results\n","         lrr:DataFrame = pd.read_pickle(last_run_path)\n","         print(lrr)\n","         best_config = lrr.iloc[0]\n","      except Exception:\n","         os.remove(last_run_path)\n","         pass\n","    \n","   if best_config is None:\n","      # Run GridSearch of all possible configurations\n","      hpg = PGrid(dict(\n","         seq_len=[15, 35],\n","         n_estimators=[1]\n","      ))\n","      \n","      #TODO: introduce a function to generate all combinations of N indicators\n","      ib = IndicatorBag()\n","      \n","      ib.add('rsi', length=[3])\n","      ib.add('zscore', length=[3])\n","      \n","      ib.add('bbands', length=[10, 15], mamode=['vwma'], std=[1.25, 1.75])\n","      # ib.add('donchian', length=[10])\n","      # ib.add('accbands', length=[10, 15])\n","      \n","      ib.add('delta_vwap')\n","      # ib.add('vwap')\n","      # ib.add('bop')\n","      # ib.add('mom')\n","      # ib.add('inertia')\n","      # ib.add('atr')\n","      #TODO add more T/A options\n","      \n","      ibcl = ls([x for x in ib.sampling(4)], True)\n","      # ibcl = ls(ib.expand(), True)\n","      print(ibcl)\n","      \n","      # TODO: convert the `expand` methods to generator functions, so that there aren't as many local functions allocated \n","      # TODO ...as there are combinations of indicator options\n","      # ibcl = ib.expand()\n","      idll = [x for x in map(lambda d: Indicators(*d.items()), ibcl)]\n","      # ito = Indicators()\n","      # ito.add('rsi', length=2)\n","      # ito.add('zscore', length=2)\n","      # ito.add('bop', length=2)\n","      # ito.add('delta_vwap')\n","      # ito.add('bbands', length=10, mamode='vwma', std=1.25)\n","      # # ito.add('accbands', length=10)\n","      \n","      # idll = [ito]\n","      hpdl = list(hpg.expand())\n","      train_symbols = [\n","         'NOC',\n","         'PAYC', 'ETSY',\n","         'GE', 'AMZN',\n","      ]\n","      \n","      from tqdm import tqdm\n","      pbar = tqdm(total=len(idll) * len(hpdl))\n","      \n","      bests = []\n","      for data_transform in idll:\n","         for hyperparams in hpdl:\n","            pbar.update()\n","            res = evaluateConfigurationOn(train_symbols, hyperparams, data_transform, val_split=0.25, n_eval_days=365)\n","            # res['']\n","            print(res)\n","            bests.append(res.asdict())\n","   \n","      explogs = DataFrame.from_records(bests)\n","      # alltimebest = maxby(bests, _['roi'])\n","      # print(alltimebest)\n","      explogs:DataFrame = explogs.sort_values('vs_market', ascending=False)\n","      # explogs.to_pickle(last_run_path)\n","      # explogs['indicators'] = explogs['indicators'].apply(lambda i: dict(i.items()))\n","      # print(explogs)\n","      \n","      # winner = explogs.iloc[0]\n","      \n","      winners = explogs.iloc[:5]\n","      print(winners)\n","      #TODO evaluate top-K winners on each symbol in the loop below, saving the configuration which worked best for each given symbol\n","      \n","   elif best_config is not None:\n","      winner = best_config\n","      \n","   \n","   else:\n","      raise Exception('unreachable')\n","   \n","   #TODO now, take the best configuration and fit MINIROCKET on an aggregate dataset\n","   #TODO ...then, evaluate that on a comprehensive list of symbols, and document performance statistics\n","   #TODO ...on each symbol individually and en aggregate\n","   winning_analyzer = Indicators(*winner.indicators)\n","   winning_params   = winner.hyperparams\n","   \n","   eval_symbols =  sample(list_stonks(), 25)\n","   \n","   tests = []\n","   #TODO: aggregate training samples into a buffer during this loop, and then train a model on the aggregate afterwards to see if performance is improved\n","   # from nn.data.core import TensorBuffer\n","   \n","   data_buffer = []\n","   \n","   for symbol in eval_symbols:\n","      try:\n","         ts_idx, X, y = samples_for2(\n","            symbol=symbol, \n","            analyze=F(winning_analyzer.apply), \n","            xcols_not=['open', 'close', 'datetime'], \n","            x_timesteps=winning_params['seq_len']\n","         )\n","         \n","         X = torch.from_numpy(X)\n","         y = torch.from_numpy(y)\n","         \n","         train_idx, train_X, train_y, test_idx, test_X, test_y = split_samples(index=ts_idx, X=X, y=y, pct=0.12, shuffle=False)\n","         train_X, test_X = tuple((v.unsqueeze(1) if v.ndim == 2 else v.swapaxes(1, 2)) for v in (train_X, test_X))\n","         # train_y = train_y[:-train_X.shape[2]]\n","         data = tuple(map(lambda v: v.numpy(), (train_X, train_y, test_X, test_y)))\n","         \n","         print(len(X))\n","         # data_buffer.append((train_X, train_y, test_X, test_y))\n","         data_buffer.append((X.unsqueeze(1).numpy() if X.ndim == 2 else X.swapaxes(1, 2).numpy(), y.numpy()))\n","         continue\n","      \n","         res:Dict[str, Any] = test_minirocket(data=data, n_estimators=1)\n","         res['symbol'] = symbol\n","         \n","         \n","         #TODO run the backtest on the model\n","         \n","         btres = backtest_minirockets(symbol, ts_idx, test_X, test_y, res['model'], n_eval_days=365)\n","         pickle.dump(res['model'], open(f'.pretrained_models/trained_for_{symbol}.pickle', 'wb+'))\n","                           \n","         dcpy(res, btres, 'roi', 'baseline_roi', 'pl_ratio', 'vs_market')\n","         tests.append(res)\n","      \n","      \n","      \n","      except ValueError as e:\n","         pprint(e)\n","         if input('(y/n)  continue?').lower().strip().startswith('n'):\n","            raise e\n","         else:\n","            continue\n","   \n","   if len(tests) > 0:\n","      tests = DataFrame.from_records(tests)\n","      tests = tests.sort_values('vs_market', ascending=False)[['symbol', 'vs_market', 'roi', 'baseline_roi', 'accuracy', 'model']]\n","      print(tests)\n","      tests.to_pickle('minirocket_results.pickle')\n","      \n","      model_efficacy = len(tests[tests.roi > 1.0])/len(tests)\n","      \n","      rois = tests.roi\n","      # P = rois[rois > 0].mean()\n","      # pl_ratio = \n","      print('model efficacy: ' + '\\n'.join(map(lambda x: f'  {x}', [\n","         f'pct-profitable: {model_efficacy*100:.3f}%',\n","         f'mean ROI:  {rois.mean()*100:3f}%',\n","      ])))\n","   \n","   #? take only most recent N-years of data, and train on data from multiple symbols\n","   trunc = lambda N, nd: nd[-N:] if N is not None else nd\n","   #* aggregate training-sample chunks\n","   # _train_X = np.concatenate([v[0].numpy() for v in data_buffer])\n","   # _train_y = np.concatenate([v[1].numpy() for v in data_buffer])\n","   # _test_X =  np.concatenate([v[2].numpy() for v in data_buffer])\n","   # _test_y =  np.concatenate([v[3].numpy() for v in data_buffer])\n","   \n","   _X = np.concatenate([x for x,_ in data_buffer])\n","   _y = np.concatenate([y for _,y in data_buffer])\n","   Nt = 12000\n","   train_X, train_y, test_X, test_y = split_samples(X=trunc(Nt, _X), y=trunc(Nt, _y), shuffle=True, pct=0.05)\n","   \n","   print(f'training MINIROCKET on {len(train_X)} samples..')\n","   res = test_minirocket(data=(train_X, train_y, test_X, test_y), n_estimators=1)\n","   pickle.dump(res, open('.res.pickle', 'wb+'))\n","   model = res['model']\n","   wmdl = partial(wrapped_minirockets, model)\n","   \n","   from faux.backtesting.multibacktester import PolySymBacktester, backtest\n","   \n","   start_date = pd.to_datetime('2022-01-01')\n","   tester, summary = backtest(eval_symbols, winning_params, winning_analyzer, wmdl, date_begin=start_date)\n","   print(summary)\n","   \n","   exit()\n","   \n","   \n","   symkey = lambda item: (Symbol(id=item[0]), expandps(item[1]))\n","   pspace = list(map(symkey, [\n","      ('zscore', dict(length=[2, 14])),\n","      ('zscore', dict(length=[2, 14])),\n","      # ('rsi', dict(length=[3, 7])),\n","      # ('bbands', dict(length=lrange(3, 7, 1), std=[0.75, 1.0, 1.25, 1.5, 2.0, 2.5, 3.0], mamode=['wma'])),\n","      # ('accbands', dict(length=[3, 7, 14], mamode=['wma'])),\n","      # ('bop', dict()),\n","      # ('intertia', dict()),\n","      # ('mom', dict(length=[3, 5, 7, 9], offset=[0, 1, 2])),\n","   ]))\n","   \n","   def wimplode(d):\n","      top = {(k.name if isinstance(k, Symbol) else str(k)):implode(v) for k, v in d.items()}\n","      return top\n","   \n","   configs = list(map(wimplode, expandps(pspace)))\n","   pprint(configs)\n","   print(len(configs), ' possible configurations')\n","   input()\n","   \n","   from tqdm import tqdm\n","   \n","   for cfg in tqdm(configs):\n","      test_indicator_config('AMZN', cfg.items())\n","      \n","def get_config_space():\n","   from faux.pgrid import PGrid\n","   from faux.features.ta.loadout import IndicatorBag, Indicators\n","   \n","   # Run GridSearch of all possible configurations\n","   hpg = PGrid(dict(\n","      seq_len=[15, 25, 35],\n","      n_estimators=[1]\n","   ))\n","   \n","   #TODO: introduce a function to generate all combinations of N indicators\n","   ib = IndicatorBag()\n","   ib.add('rsi', length=[2])\n","   ib.add('zscore', length=[2])\n","   ib.add('bbands', length=[10], mamode=['vwma'], std=[1.25])\n","   ib.add('delta_vwap')\n","   \n","   ib.add('stoch')\n","   # ib.add('atr')\n","   ib.add('obv')\n","   \n","   ibcl = ls([x for x in ib.sampling(5)], True)\n","   \n","   feature_specs = [x for x in map(lambda d: Indicators(*d.items()), ibcl)]\n","   random.shuffle(feature_specs)\n","   param_specs = list(hpg.expand())\n","   \n","   return param_specs, feature_specs[:18]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'SBUX': {'hyperparameters': {'n_estimators': 1, 'seq_len': 35},\n","          'indicators': <fn.func.F object at 0x7ff18b31a0a0>,\n","          'model': Pipeline(steps=[('minirocketmultivariate', MiniRocketMultivariate()), ('ridgeclassifiercv', RidgeClassifierCV(alphas=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])))]),\n","          'model_ctor': <class 'tsai.models.MINIROCKET.MiniRocketClassifier'>,\n","          'model_ctor_args': None}}\n","\u001b[4m\u001b[34m987129252.py:98\u001b[0m [<fn.func.F object at 0x7ff18b31a0a0>]\n","\u001b[4m\u001b[34mmultibacktester.py:497\u001b[0m [<fn.func.F object at 0x7ff18b31a0a0>]\n"]},{"ename":"AttributeError","evalue":"'F' object has no attribute 'apply'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 105\u001b[0m\n\u001b[1;32m    101\u001b[0m    \u001b[39mreturn\u001b[39;00m models\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    104\u001b[0m    \u001b[39m# main()\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m    polyfinetune()\n","Cell \u001b[0;32mIn[20], line 99\u001b[0m, in \u001b[0;36mpolyfinetune\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfaux\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbacktesting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmultibacktester\u001b[39;00m \u001b[39mimport\u001b[39;00m PolySymBacktester, backtest\n\u001b[1;32m     98\u001b[0m \u001b[39mprint\u001b[39m(transforms)\n\u001b[0;32m---> 99\u001b[0m backtest(symbols, params\u001b[39m=\u001b[39;49mparams, transforms\u001b[39m=\u001b[39;49mtransforms, model\u001b[39m=\u001b[39;49mmodels, date_begin\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m2022-01-01\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m models\n","File \u001b[0;32m~/Documents/Development/fauxnet_next/faux/backtesting/multibacktester.py:499\u001b[0m, in \u001b[0;36mbacktest\u001b[0;34m(symbols, params, transforms, model, dir, date_begin, date_end)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mprint\u001b[39m(transforms)\n\u001b[1;32m    498\u001b[0m transforms \u001b[39m=\u001b[39m [(f[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m transforms]\n\u001b[0;32m--> 499\u001b[0m samples \u001b[39m=\u001b[39m [samples_from(df, params, transform, date_begin\u001b[39m=\u001b[39mdate_begin, date_end\u001b[39m=\u001b[39mdate_end) \u001b[39mfor\u001b[39;00m df, transform \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(frames, transforms)]\n\u001b[1;32m    501\u001b[0m tester \u001b[39m=\u001b[39m PolySymBacktester(symbols, datasets\u001b[39m=\u001b[39mframes, samples\u001b[39m=\u001b[39msamples, models\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m    502\u001b[0m summary \u001b[39m=\u001b[39m tester\u001b[39m.\u001b[39mrun()\n","File \u001b[0;32m~/Documents/Development/fauxnet_next/faux/backtesting/multibacktester.py:499\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mprint\u001b[39m(transforms)\n\u001b[1;32m    498\u001b[0m transforms \u001b[39m=\u001b[39m [(f[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m transforms]\n\u001b[0;32m--> 499\u001b[0m samples \u001b[39m=\u001b[39m [samples_from(df, params, transform, date_begin\u001b[39m=\u001b[39;49mdate_begin, date_end\u001b[39m=\u001b[39;49mdate_end) \u001b[39mfor\u001b[39;00m df, transform \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(frames, transforms)]\n\u001b[1;32m    501\u001b[0m tester \u001b[39m=\u001b[39m PolySymBacktester(symbols, datasets\u001b[39m=\u001b[39mframes, samples\u001b[39m=\u001b[39msamples, models\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m    502\u001b[0m summary \u001b[39m=\u001b[39m tester\u001b[39m.\u001b[39mrun()\n","File \u001b[0;32m~/Documents/Development/fauxnet_next/faux/backtesting/multibacktester.py:74\u001b[0m, in \u001b[0;36msamples_from\u001b[0;34m(df, params, transform, date_begin, date_end)\u001b[0m\n\u001b[1;32m     69\u001b[0m    transform \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x\n\u001b[1;32m     71\u001b[0m symbol \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mname\n\u001b[1;32m     72\u001b[0m ts_idx, X, y \u001b[39m=\u001b[39m samples_for(\n\u001b[1;32m     73\u001b[0m    symbol\u001b[39m=\u001b[39mdf, \n\u001b[0;32m---> 74\u001b[0m    analyze\u001b[39m=\u001b[39mF(transform\u001b[39m.\u001b[39;49mapply), \n\u001b[1;32m     75\u001b[0m    xcols_not\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     76\u001b[0m    x_timesteps\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m idx:pd\u001b[39m.\u001b[39mDatetimeIndex \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDatetimeIndex([d\u001b[39m.\u001b[39mdate() \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m ts_idx])\n\u001b[1;32m     80\u001b[0m \u001b[39m# print(idx)\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'F' object has no attribute 'apply'"]}],"source":["def polyfinetune():\n","   from faux.features.ta.loadout import Indicators\n","   # from faux.backtesting.multibacktester import samples_from\n","   pofn = 'symbol_optima.pickle'\n","   pretrained_model_file = 'models_finetuned.pickle'\n","   initial_tunings_file = '.initial_tunings.pickle'\n","   \n","   if P.exists(pretrained_model_file):\n","      models:Dict[str, Any] = pickle.load(open(pretrained_model_file, 'rb'))\n","      symbols = list(models.keys())\n","   \n","   else:\n","      if P.exists(pofn):\n","         something_mhmm = pickle.load(open(pofn, 'rb'))\n","      \n","      if P.exists(initial_tunings_file):\n","         symbols, pspecs, fspecs = pickle.load(open(initial_tunings_file, 'rb'))\n","         pspecs = pspecs[0:1]\n","         optima = {}\n","      \n","      else:\n","         optima = {}\n","         symbols = sample(list_stonks(), 20)\n","         pspecs, fspecs = get_config_space()\n","         print(fspecs)\n","         print(len(fspecs))\n","         \n","         results = finetune_for(symbols[0], pspecs, fspecs, model_ctor=None, val_split=0.05, n_eval_days=100)\n","         for r in results:\n","            print(tuple(r.values()))\n","         # input()\n","         \n","         pspecs = [r['hyperparams'] for r in results]\n","         fspecs = [r['transform'] for r in results]\n","         pspecs = pspecs[0:1]\n","         \n","         optima[symbols[0]] = (pspecs[0], results[0]['transform'])\n","         pickle.dump((symbols, pspecs, fspecs), open(initial_tunings_file, 'wb+'))\n","         # input()\n","      \n","      failures = dict()\n","      for i in range(0, len(symbols)):\n","         symbol = symbols[i]\n","         \n","         try:\n","            print(f'TUNING OPTIMA for {symbol}')\n","            sym_results = finetune_for(symbol, pspecs, fspecs, val_split=0.05, n_eval_days=365)\n","            optima[symbol] = (sym_results[0]['hyperparams'], sym_results[0]['indicators'])\n","         \n","         except Exception as error:\n","            optima[symbol] = None\n","            failures[symbol] = error\n","            continue\n","         \n","         break\n","      \n","      pickle.dump(optima, open(pofn, 'wb+'))\n","      pprint(failures)\n","      print('OPTIMA-TUINING COMPLETE')\n","      \n","      models = {}\n","      for sym in symbols:\n","         if (sym not in optima or optima[sym] is None):\n","            continue\n","         \n","         (pspec, ispecs) = optima[sym]\n","         print(type(ispecs).__qualname__)\n","         \n","         tspec = ispecs if callable(ispecs) else Indicators(*ispecs)\n","         \n","         res = cfgOptStep(sym, pspec, tspec, val_split=0.15, n_eval_days=365, unlink=False, backtest=False)\n","         model = res['model']\n","         \n","         models[sym] = dict(\n","            model_ctor=type(model),\n","            model_ctor_args=(model.get_config() if hasmethod(model, 'get_config') else None),\n","            model=model,\n","            indicators=ispecs,\n","            hyperparameters=pspec\n","         )\n","         \n","      pickle.dump(models, open(pretrained_model_file, 'wb+'))\n","      \n","   assert models is not None\n","   \n","   #* load models\n","   model_entries = models\n","   pprint(model_entries)\n","   \n","   models = {sym:(model_entries[sym]['model']) for sym in symbols if (sym in model_entries and model_entries[sym] is not None)}\n","   params = [model_entries[sym] for sym in symbols if (sym in model_entries and model_entries[sym] is not None)]\n","   params = [p.get('hyperparameters', None) for p in params]\n","   transforms = [model_entries[sym]['indicators'] for sym in symbols if (sym in model_entries)]\n","   transforms = list(map(lambda t: Indicators(*t) if isiterable(t) else t, transforms))\n","   \n","   from faux.backtesting.multibacktester import PolySymBacktester, backtest\n","   \n","   print(transforms)\n","   backtest(symbols, params=params, transforms=transforms, model=models, date_begin='2022-01-01')\n","                  \n","   return models\n","   \n","if __name__ == '__main__':\n","   # main()\n","   polyfinetune()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":2}
