{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, tsai, builtins\n",
    "from torch import Tensor\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from cytoolz.dicttoolz import valmap, valfilter, itemfilter, itemmap, merge\n",
    "from typing import *\n",
    "from faux.features.ta.loadout import Indicators, bind_indicator\n",
    "from functools import *\n",
    "from cytoolz.functoolz import *\n",
    "import cytoolz.functoolz as ft\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "\n",
    "\n",
    "def transform_json(json_data, variables={}, functions={}, path=[]):\n",
    "    if isinstance(json_data, dict):\n",
    "        return {k: transform_json(v, variables, functions, path + [k]) for k, v in json_data.items()}\n",
    "    elif isinstance(json_data, list):\n",
    "        return [transform_json(v, variables, functions, path + [i]) for i, v in enumerate(json_data)]\n",
    "    elif isinstance(json_data, str):\n",
    "        if \"${\" in json_data and \"}\" in json_data:\n",
    "            return transform_json(json_data.format(**valmap(str, variables)))\n",
    "        elif json_data == \"?\":\n",
    "            return transform_json(input(\"Enter value for {}:\".format(\".\".join(map(str, path)))), variables, functions, path)\n",
    "        elif json_data.startswith(\"`\") and json_data.endswith(\"`\"):\n",
    "            expr = json_data[1:len(json_data)-1]\n",
    "            #TODO evaluate by parsing the expression to allow for some magic\n",
    "            # print(json_data[1:][:-1]\n",
    "            # return eval(expr, merge(functions, variables))\n",
    "            return json_data\n",
    "        else:\n",
    "            return json_data\n",
    "    else:\n",
    "        return json_data\n",
    "     \n",
    "def parse_feature_spec(spec: List[Any])->Callable[[DataFrame], DataFrame]:\n",
    "   analysis_funcs = list(map(parse_feature_spec_item, spec))\n",
    "   \n",
    "   def wfunc(df: DataFrame)->DataFrame:\n",
    "      return reduce(lambda df, fn: fn(df), analysis_funcs, df)\n",
    "   \n",
    "   return wfun\n",
    "\n",
    "def lookup_fn(fn_lookup, fn_id):\n",
    "   try:\n",
    "      fn = bind_indicator(fn_id)\n",
    "      return fn\n",
    "   except Exception as error:\n",
    "      pass\n",
    "   raise KeyError(fn_id)\n",
    "\n",
    "def bind_item_fn(func, configuration={}):\n",
    "   @wraps(func)\n",
    "   def wfunc(df:DataFrame, *args, **kwargs):\n",
    "      return func(df, *args, **kwargs)\n",
    "   return wfunc\n",
    "\n",
    "class TAFnLookup:\n",
    "   def __init__(self) -> None:\n",
    "      self.cache = dict()\n",
    "      \n",
    "   def mount_pinescript(self, pinescript_or_filename:str):\n",
    "      \"\"\" expose the definitions in the given pinescript as functions \"\"\"\n",
    "      #TODO\n",
    "      return None\n",
    "      \n",
    "   def __getitem__(self, fn_id:str):\n",
    "      # raise Exception(fn_id)\n",
    "      if not isinstance(fn_id, str):\n",
    "         return None\n",
    "      \n",
    "      elif fn_id in self.cache:\n",
    "         return self.cache[fn_id]\n",
    "      elif isinstance(fn_id, str) and hasattr(ta, fn_id):\n",
    "         ta_fn = getattr(ta, fn_id)\n",
    "         if callable(ta_fn):\n",
    "            self.cache[fn_id] = ta_fn\n",
    "            return ta_fn\n",
    "      elif isinstance(fn_id, str) and hasattr(pd.Series, fn_id):\n",
    "         series_method = getattr(pd.Series, fn_id)\n",
    "         if callable(series_method):\n",
    "            self.cache[fn_id] = series_method\n",
    "            return series_method\n",
    "      return None\n",
    "\n",
    "fn_lookup = TAFnLookup()\n",
    "import re\n",
    "\n",
    "_uid = [0]\n",
    "def anoncolid():\n",
    "   # nonlocal _uid\n",
    "   name = f'Unnamed {_uid[0]}'\n",
    "   _uid[0] += 1\n",
    "   return name\n",
    "\n",
    "def parse_feature_spec_item(item:Any)->Callable[[DataFrame], DataFrame]:\n",
    "   type_name = type(item).__qualname__\n",
    "   if isinstance(item, str):\n",
    "      if (item[0], item[-1]) == ('`', '`'):\n",
    "         expr = item[1:-1]\n",
    "         \n",
    "         #TODO when the expr-string represents a column-assignment, return an eval_function that invokes eval inline\n",
    "         assign_pat = re.compile(r'^\\s*([a-zA-Z_]\\w*)\\s*=\\s*')\n",
    "         m = assign_pat.match(expr)\n",
    "         if m is not None:\n",
    "            #* Named assignments\n",
    "            def eval_fn(df: DataFrame):\n",
    "               print(f'Evaluating {expr} on DataFrame..')\n",
    "               df = df.copy()\n",
    "               df.eval(expr, inplace=True)#, resolvers=(fn_lookup,))\n",
    "               return df\n",
    "            return eval_fn\n",
    "         \n",
    "         def eval_fn(df: DataFrame)->DataFrame:\n",
    "            print(f'Evaluating {expr} on DataFrame..')\n",
    "            result = df.eval(expr)#, resolvers=(fn_lookup,))\n",
    "            df[anoncolid()] = result\n",
    "            return df\n",
    "         \n",
    "         return eval_fn\n",
    "      \n",
    "   elif isinstance(item, list):\n",
    "      if len(item) == 1:\n",
    "         raise NotImplemented()\n",
    "      \n",
    "   elif isinstance(item, dict):\n",
    "      if len(item) == 1:\n",
    "         item_ident, item_args = next(iter(item.items()))\n",
    "         item_fn = lookup_fn(fn_lookup, item_ident)\n",
    "         item_wfn = bind_item_fn(item_fn, item_args)\n",
    "         return item_wfn\n",
    "      \n",
    "   elif callable(item):\n",
    "      return item\n",
    "   \n",
    "   raise ValueError(f'Unhandled {item}')\n",
    "\n",
    "the_functions = merge(builtins.__dict__, {})\n",
    "# print('cunt-lips:'[-1])\n",
    "# input()\n",
    "\n",
    "from faux.backtesting.common import load_frame\n",
    "from tools import dotget, dotgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[34m1562758150.py:26\u001b[0m ['`close.pct_change()`', '`close.pct_change().zscore()`', '`pissAndTitties = (high - low)`', '`titsAndPiss=pissAndTitties.zscore(30)`']\n",
      "\u001b[4m\u001b[34m1562758150.py:41\u001b[0m <__main__.Analysis object at 0x7f141431db80>\n",
      "\u001b[4m\u001b[34m3002217508.py:115\u001b[0m Evaluating close.pct_change() on DataFrame..\n",
      "\u001b[4m\u001b[34m3002217508.py:115\u001b[0m Evaluating close.pct_change().zscore() on DataFrame..\n",
      "\u001b[4m\u001b[34m3002217508.py:108\u001b[0m Evaluating pissAndTitties = (high - low) on DataFrame..\n",
      "\u001b[4m\u001b[34m3002217508.py:108\u001b[0m Evaluating titsAndPiss=pissAndTitties.zscore(30) on DataFrame..\n",
      "\u001b[4m\u001b[34m1562758150.py:37\u001b[0m \n",
      "                                datetime    open    high     low   close  \\\n",
      "datetime                                                                  \n",
      "2013-01-28 15:00:00 2013-01-28 15:00:00   15.64   16.19   15.57   16.07   \n",
      "2013-01-29 15:00:00 2013-01-29 15:00:00   16.38   16.44   16.15   16.37   \n",
      "2013-01-30 15:00:00 2013-01-30 15:00:00   16.32   16.52   16.23   16.32   \n",
      "2013-01-31 15:00:00 2013-01-31 15:00:00   16.32   16.40   16.25   16.27   \n",
      "2013-02-01 15:00:00 2013-02-01 15:00:00   16.40   16.41   16.01   16.20   \n",
      "...                                 ...     ...     ...     ...     ...   \n",
      "2023-01-23 15:00:00 2023-01-23 15:00:00  138.12  143.32  137.90  141.11   \n",
      "2023-01-24 15:00:00 2023-01-24 15:00:00  140.31  143.16  140.30  142.53   \n",
      "2023-01-25 15:00:00 2023-01-25 15:00:00  140.89  142.43  138.81  141.86   \n",
      "2023-01-26 15:00:00 2023-01-26 15:00:00  143.17  144.25  141.90  143.96   \n",
      "2023-01-27 15:00:00 2023-01-27 15:00:00  143.16  147.23  143.08  145.93   \n",
      "\n",
      "                        volume  Unnamed 10  Unnamed 11  pissAndTitties  \\\n",
      "datetime                                                                 \n",
      "2013-01-28 15:00:00  785515724         NaN         NaN            0.62   \n",
      "2013-01-29 15:00:00  571072068    0.018668         NaN            0.29   \n",
      "2013-01-30 15:00:00  417153268   -0.003054         NaN            0.29   \n",
      "2013-01-31 15:00:00  319332860   -0.003064         NaN            0.15   \n",
      "2013-02-01 15:00:00  539468356   -0.004302         NaN            0.40   \n",
      "...                        ...         ...         ...             ...   \n",
      "2023-01-23 15:00:00   81760313    0.023500    1.192830            5.42   \n",
      "2023-01-24 15:00:00   66435142    0.010063    0.508755            2.86   \n",
      "2023-01-25 15:00:00   65799349   -0.004701   -0.247145            3.62   \n",
      "2023-01-26 15:00:00   54105068    0.014803    0.758402            2.35   \n",
      "2023-01-27 15:00:00   70555843    0.013684    0.684678            4.15   \n",
      "\n",
      "                     titsAndPiss  \n",
      "datetime                          \n",
      "2013-01-28 15:00:00          NaN  \n",
      "2013-01-29 15:00:00          NaN  \n",
      "2013-01-30 15:00:00          NaN  \n",
      "2013-01-31 15:00:00          NaN  \n",
      "2013-02-01 15:00:00          NaN  \n",
      "...                          ...  \n",
      "2023-01-23 15:00:00     1.334981  \n",
      "2023-01-24 15:00:00    -0.883317  \n",
      "2023-01-25 15:00:00    -0.194115  \n",
      "2023-01-26 15:00:00    -1.241646  \n",
      "2023-01-27 15:00:00     0.355769  \n",
      "\n",
      "[2519 rows x 10 columns]\n",
      "\u001b[4m\u001b[34m1562758150.py:43\u001b[0m \n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [2013-01-28 15:00:00, 2013-01-29 15:00:00, 2013-01-30 15:00:00, 2013-01-31 15:00:00, 2013-02-01 15:00:00, 2013-02-04 15:00:00, 2013-02-05 15:00:00, 2013-02-06 15:00:00, 2013-02-07 15:00:00, 2013-02-08 15:00:00, 2013-02-11 15:00:00, 2013-02-12 15:00:00, 2013-02-13 15:00:00, 2013-02-14 15:00:00, 2013-02-15 15:00:00, 2013-02-19 15:00:00, 2013-02-20 15:00:00, 2013-02-21 15:00:00, 2013-02-22 15:00:00, 2013-02-25 15:00:00, 2013-02-26 15:00:00, 2013-02-27 15:00:00, 2013-02-28 15:00:00, 2013-03-01 15:00:00, 2013-03-04 15:00:00, 2013-03-05 15:00:00, 2013-03-06 15:00:00, 2013-03-07 15:00:00, 2013-03-08 15:00:00, 2013-03-11 15:00:00, 2013-03-12 15:00:00, 2013-03-13 15:00:00, 2013-03-14 15:00:00, 2013-03-15 15:00:00, 2013-03-18 15:00:00, 2013-03-19 15:00:00, 2013-03-20 15:00:00, 2013-03-21 15:00:00, 2013-03-22 15:00:00, 2013-03-25 15:00:00, 2013-03-26 15:00:00, 2013-03-27 15:00:00, 2013-03-28 15:00:00, 2013-04-01 15:00:00, 2013-04-02 15:00:00, 2013-04-03 15:00:00, 2013-04-04 15:00:00, 2013-04-05 15:00:00, 2013-04-08 15:00:00, 2013-04-09 15:00:00, 2013-04-10 15:00:00, 2013-04-11 15:00:00, 2013-04-12 15:00:00, 2013-04-15 15:00:00, 2013-04-16 15:00:00, 2013-04-17 15:00:00, 2013-04-18 15:00:00, 2013-04-19 15:00:00, 2013-04-22 15:00:00, 2013-04-23 15:00:00, 2013-04-24 15:00:00, 2013-04-25 15:00:00, 2013-04-26 15:00:00, 2013-04-29 15:00:00, 2013-04-30 15:00:00, 2013-05-01 15:00:00, 2013-05-02 15:00:00, 2013-05-03 15:00:00, 2013-05-06 15:00:00, 2013-05-07 15:00:00, 2013-05-08 15:00:00, 2013-05-09 15:00:00, 2013-05-10 15:00:00, 2013-05-13 15:00:00, 2013-05-14 15:00:00, 2013-05-15 15:00:00, 2013-05-16 15:00:00, 2013-05-17 15:00:00, 2013-05-20 15:00:00, 2013-05-21 15:00:00, 2013-05-22 15:00:00, 2013-05-23 15:00:00, 2013-05-24 15:00:00, 2013-05-28 15:00:00, 2013-05-29 15:00:00, 2013-05-30 15:00:00, 2013-05-31 15:00:00, 2013-06-03 15:00:00, 2013-06-04 15:00:00, 2013-06-05 15:00:00, 2013-06-06 15:00:00, 2013-06-07 15:00:00, 2013-06-10 15:00:00, 2013-06-11 15:00:00, 2013-06-12 15:00:00, 2013-06-13 15:00:00, 2013-06-14 15:00:00, 2013-06-17 15:00:00, 2013-06-18 15:00:00, 2013-06-19 15:00:00, ...]\n",
      "\n",
      "[2519 rows x 0 columns]\n"
     ]
    }
   ],
   "source": [
    "json_data = {\n",
    "   'name': 'Example (Proto)Strategy #1',\n",
    "   'description': 'This is a test',\n",
    "   \n",
    "   'data': {\n",
    "      'features': [\n",
    "         # '`vol_delta=volume.pct_change()`',\n",
    "         # '`z_close_delta = close.pct_change().zscore()`',\n",
    "         # '`trend_a = close.pct_change().sma(5).zscore()`',\n",
    "         # {'zscore': {'close': 'vol_delta'}}\n",
    "         '`close.pct_change()`',\n",
    "         '`close.pct_change().zscore()`',\n",
    "         '`pissAndTitties = (high - low)`',\n",
    "         '`titsAndPiss=pissAndTitties.zscore(30)`'\n",
    "      ]\n",
    "   }\n",
    "}\n",
    "\n",
    "for name in dir(ta):\n",
    "   val = getattr(ta, name)\n",
    "   if callable(val) and not hasattr(pd.Series, name):\n",
    "      setattr(pd.Series, name, val)\n",
    "\n",
    "df = load_frame('AAPL', './stonks')\n",
    "features = transform_json(json_data)['data']['features']\n",
    "print(features)\n",
    "\n",
    "class Analysis:\n",
    "   def __init__(self, *items):\n",
    "      self.indicators = list(map(parse_feature_spec_item, items))\n",
    "      self._fn = (lambda df:  reduce(lambda df, fn: fn(df), self.indicators, df))\n",
    "      \n",
    "   def __call__(self, df:DataFrame, **kwargs)->DataFrame:\n",
    "      input_df:DataFrame = df.copy()\n",
    "      output_df:DataFrame = self._fn(input_df)\n",
    "      # print(input_df)\n",
    "      print(output_df)\n",
    "      return output_df\n",
    "\n",
    "feat = Analysis(*features)\n",
    "print(feat)\n",
    "\n",
    "print(feat(df)[[]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
